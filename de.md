What is asked: Python, SQL, spark, data modelling

1. Big data, Distributed systems -> Spark, Hadoop

System design:
- How to manage the whole data pipeline/ data flow/ ETL
- Data quality management

2. Hands on Cloud - AWS

What to do?
Collect, manage store, make available data for analysis

Types of data:
Batch data/Streaming data(real time)


Syllabus:

📦 𝗘𝘁𝗹 𝗧𝗮𝘀𝗸𝘀  
- Data extraction from APIs/DB  
- Data transformation  
- Automating pipelines with scripts  

📌 𝗟𝗶𝘀𝘁 𝗖𝗼𝗺𝗽𝗿𝗲𝗵𝗲𝗻𝘀𝗶𝗼𝗻𝘀 & 𝗟𝗮𝗺𝗯𝗱𝗮𝘀  
- Clean one-liners for filtering and transformation  

🧱 𝗗𝗮𝘁𝗮𝗯𝗮𝘀𝗲 𝗜𝗻𝘁𝗲𝗴𝗿𝗮𝘁𝗶𝗼𝗻  
- Connect Python to SQL (using sqlite3, psycopg2, SQLAlchemy)  
- Run queries from scripts  

📂 𝗢𝗢𝗣 𝗕𝗮𝘀𝗶𝗰𝘀  
- Classes & Objects  
- Inheritance, Encapsulation (for code structure in projects)

🛠 𝗪𝗼𝗿𝗸𝗳𝗹𝗼𝘄 𝗔𝘂𝘁𝗼𝗺𝗮𝘁𝗶𝗼𝗻  
- Scheduling jobs (cron, schedule, Airflow basics)  
- Scripting repetitive tasks  

🧠 𝗜𝗻𝘁𝗲𝗿𝘃𝗶𝗲𝘄-𝗙𝗼𝗰𝘂𝘀𝗲𝗱 𝗣𝗿𝗮𝗰𝘁𝗶𝗰𝗲  
- Python questions on LeetCode  
- Data cleaning projects  
- Building mini ETL pipelines  

💥 Must-Have Skills:  
- Clean code and logic clarity  
- Data manipulation with pandas
- Building end-to-end scripts  

